# ProBE: A Reliable Benchmarking Pipeline for AFP Models

<!-- Optional: Add badges here for CI/CD, License, DOI -->
[![Singularity](https://img.shields.io/badge/Singularity-v1.0-blue.svg)](link-to-release)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE.txt)
[![DOI](https://zenodo.org/badge/1045655066.svg)](https://doi.org/10.5281/zenodo.16980307)

ProBE is a reproducible pipeline for the complete benchmarking of Automatic Function Prediction (AFP) tools. It constructs a time-resolved ground truth from Gene Ontology (GO) annotations, preprocesses model predictions, and evaluates performance using metrics from the CAFA challenges.

---

## Table of Contents

* [Overview](#overview)
* [Quick Start Guide (Singularity)](#quick-start-guide-singularity)
* [Typical Workflow](#typical-workflow)
* [Detailed Documentation](#detailed-documentation)
  * [Python Installation](#python-installation)
  * [Required Data](#required-data)
  * [Project Directory Structure](#project-directory-structure)
  * [Configuration (`config.yaml`)](#configuration-configyaml)
  * [Pipeline Commands](#pipeline-commands)
  * [Prediction File Format](#prediction-file-format)
  * [Multiple Tool Prediction](#multiple-tool-prediction-on-the-same-ground-truth)
  * [CLI Override](#cli-override)
  * [How to Make Predictions](#how-to-make-predictions)
  * [Typical Use Case [Python Run]](#typical-use-case-python-run)
  * [How ProBE Works](#how-probe-works)
  * [Pipeline Process](#pipeline-process)
  * [Repository Structure](#repository-structure)
  * [Components Description](#components-description)
* [Citing this Work](#citing-this-work)
* [Acknowledgments](#acknowledgments)
* [License](#license)



## Overview

ProBE simulates a miniature CAFA challenge by comparing GO annotations between two points in time. This allows you to benchmark your AFP tool against newly acquired experimental annotations.

**Key Features:**
*   **Automated Ground Truth Creation:** Generates a reliable benchmark set based on novel experimental evidence (NK/LK annotations).
*   **Reproducible:** Guarantees the same ground truth from the same initial conditions, powered by Singularity.
*   **Flexible:** Allows the use of custom Gene Ontology Annotation (GOA) files.
*   **Comprehensive Benchmarking:** Calculates a wide range of metrics (Precision, Recall, F1, S-measure, ROC curves) and provides detailed reports and graphs.



## Quick Start Guide (Singularity)

This guide will get you running the first step of the pipeline in 5 minutes. This is the recommended method for all users.

**Prerequisites:** You must have [Singularity](https://sylabs.io/docs/) (version 3.5+) installed.

**1. Get the Pipeline:**
Clone this repository and download the container image from our GitHub Releases.
```bash
# Clone the repository to get templates and scripts
git clone https://github.com/ggradara/ProBE.git
cd ProBE

# Download the container from the latest GitHub Release page
wget https://github.com/ggradara/ProBE/releases/download/v1.0.0/probe.sif
```

Alternatively, you can also build the singularity image yourself using the singularity.def file, with the following command:

```bash
singularity build --fakeroot probe.sif singularity.def
```


**2. Set Up Your Project::**
Clone this repository and download the container image from our GitHub Releases.

```bash
# Copy the configuration template
cp configs/config.template.yaml configs/config.yaml

# Create directories for your data
mkdir -p data/{tmp,owl_data,dmnd_dbs}
mkdir results
```

**4. Run the First Step:**
Execute the make_db command to build the [DIAMOND](https://github.com/bbuchfink/diamond) database.

```bash
singularity run \
  -B $(pwd):/workspace \
  probe.sif make_db --config /workspace/configs/config.yaml
```

- -B $(pwd):/workspace: This mounts your current project directory into the container.

- --config /workspace/config.yaml: This tells the pipeline where to find your config file inside the container.

The pipeline is now running. Your DIAMOND database will be created in the location specified in your config file. See the [Typical Workflow](#typical-workflow) section to continue.


## Typical Workflow

ProBE is a multi-step pipeline. A typical use case follows this sequence of commands:

- make_db: Create the DIAMOND database from a UniProt FASTA file.

- pc4 / pc5: Generate the CAFA4 and/or CAFA5 ground truth datasets.

- (You): Use the FASTA files generated by the pipeline (results/results_{run_tag}/preprocessing_data/fasta_seqs_for_preds_C*.fasta) to run predictions with your own AFP tool.

- preprocess: Format your tool's raw predictions to be compatible with ProBE.

- bench_general / bench_by_aspect: Run the final benchmark and generate performance reports and graphs.

**It is critical to use the same run_tag in your config.yaml for all related steps of a single analysis.**


## Detailed Documentation
### Python Installation

The recommended method is to use the pre-built Singularity container. For developers who wish to modify the source code, a local Python environment can be set up:

```bash
# Create a virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Required Data

ProBE requires the following data files, which you must download yourself:

- Gene Ontology Annotation (GOA) Files: Gzipped GAF files containing GO annotations. You will need a "new" and "old" version to create the time-resolved ground truth. Available from the [GO Consortium Archives](https://release.geneontology.org/).

- Gene Ontology (go.owl): The ontology structure file. Use the version corresponding to your new_goa file. Available from the [GO Consortium Archives](https://release.geneontology.org/).

- UniProt Knowledgebase (fasta): A FASTA file of protein sequences. For maximum coverage, we recommend concatenating UniProt SwissProt and TrEMBL. Available from [UniProt](https://www.uniprot.org/uniprotkb).

- You must also download the diamond_data directory and its contents from the newest release and make it available for the pipeline, providing its path in the config file.


### Configuration (config.yaml)

ProBE pulls its configurations from a .yaml file, a template of the file can be found in this repository. I suggest you to copy and rename it to keep track of the different options during each run, this also makes each run perfectly reproducible.
The config file has 6 different parts, one that is used by multiple commands and the others that are command specific.

Whenever you insert a path, please **use the absolute path**, this will minimize pathing conflicts.

- `shared`:
    - `new_goa`: the path of the gzipped GOA containing the new annotations
    - `go_owl`: path to the go.owl of the most recent GOA (new_goa) [str]
    - `base_path`: bind-mounted project directory [str] (Relevant only if containerized, if this is not the case it can be any directory you want.)
    - `run_path`: name of the individual run (Directory that will contain all the results of the computations) [str]
    - `dmnd_data_path`: directory which contains the chunked and whole query fasta of CAFA 4 and CAFA 5 [str]
    - `keep_tmp`: keep the temporary files, preventing their deletion after the run. This is mandatory if we are running pc4 and pc5 with the same run name at the same time. The two runs will not have the same computation time so one will delete the directory while the other is still relying on it. [bool]
    - `run_tag`: the unique tag of your run. Related runs (those which share data and processes with eachother) must have the same tag to allow the interaction of different elements of the pipeline. If none is provided a random one will be used. [str]
    - `chunk_size`: some processes require the chunking of big dataframes. You can control the use of memory resources with this option (default: 5000000) [int]
- `make_db`:
    - `db_source`: the path to the gzipped diamond db created from the new Uniprot data. Consider adding together the TrEMBl and SwissProt databases to reach maximum coverage. [str]
    - `db_name`: the name that will be given to the diamond database. [str]
- `pipeline_c4`:
    - `old_goa_C4`: GOA containing the past annotations (maybe the same or different from the C5 process). [str]
    - `preferred_threshold`: 0.2 ???? Non ricordo esattamente riguardare! [float 0-1]
    - `cores`: How many cores to use, during the DIAMOND search [int]
    - `stop_at_filtering`: stop the pipeline right before the implementation of the 'preferred threshold' this way the user may choose to evaluate the preliminary data () and choose the preferred threshold on their own. However the default is good for most uses. [bool]
    - `skip_to_filtering`: resume the pipeline to the application of the filter, once the user choose the preferred_threshold [bool]
- `pipeline_c5`:
    - `old_goa_C5`: GOA containing the past annotations (maybe the same or different from the C4 process) [str]
    - `preferred_threshold`: max allowed ratio of lost annotations [default: 0.2, float 0-1]
    - `cores`: How many cores to use, during the DIAMOND search [int]
    - `stop_at_filtering`: stop the pipeline right before the implementation of the 'preferred threshold' this way the user may choose to evaluate the preliminary data () and choose the preferred threshold on their own. However the default is good for most uses. [bool]
    - `skip_to_filtering`: resume the pipeline to application of the filter, once the user choose the preferred_threshold [bool]
- `preprocessing`:
    - `model_path`: path to the **raw** predictions of the tool  (The exact specifications will be in the designated section) The script will recognize automatically if the preiction are for CAF 4, CAFA 5 or both and behave accordingly. [str]
    - `model_name`: Name of the tool [str]
    - `propagate`: Propagate the annotations during the preprocessing step [bool]
- benchmarking:
    - `model_path_C5`: path to the **preprocessed** predictions of the tool for the C5 dataset [str]
    - `model_path_C4`: path to the **preprocessed** predictions of the tool for the C4 dataset [str]
    - `model_name`: Name of the tool [str]
    - `stepsize`: size of the steps used during the thresholding  [float 0-1]
    - `cafa`: which cafa dataset to use during the benchmarking [C4, C5, both]

All pipeline behavior is controlled by a .yaml file specified with --config. A template can be found in configs/config.template.yaml.

### Pipeline Commands
The pipeline recognizes these commands:
- `make_db`: makes the diamond database from the Uniprot database. Making the diamond search possible.
- `pc4`: creates the ground truth necessary to run the benchmark using the C4 dataset.
- `pc5`: creates the ground truth necessary to run the benchmark using the C5 dataset.
- `preprocess`: preprocesses the predictions from a function prediction prediction algorithm to make them compatible with the benchmark
- `bench_by_aspect`: compares the benchmark to the ground truth to compute the benchmark. It does not discriminate between Aspects computing only the general metrics.
- `bench_general`: compares the benchmark to the ground truth to compute the benchmark. It discriminates between Aspects, computing a different and indipendent metric for each..

For each command the only mandatory argument is --config followed by the path of the config file. The config contains in itself all the required data used to run any command.

Example of an instruction that runs pc4:

```python
python main.py pc4 --config configs/config.template.yaml
```


### Prediction File Format

predictions.tsv is the file containing the predictions of the tool that must be evaluated. The file must be in tsv format and have four columns: Query_ID, GO_ID, Ontology and Score.
- Query_ID: contains the name (ID) of the evaluated protein. It must be the same as the one with the fasta file itself.
- GO_ID: the unique identifier of the predicted GO. The allowed formats are GO:0123456 and GO_01235456, no other format will be recognized by the pipeline.
- Ontology: which ontology does the GO belongs to. It is necessary to evaluate the aspect based performance. There are three possible ontologies and are recognized as such (no other format will be recognized):
    - Cellular Components: Cellular Component/CC/C
    - Biological Processes: Biological Process/BP/B/P
    - Molecular Functions: Molecular Function/MF/F/M
- Score: the score of the individual prediction. It must be a float between 0 and 1. If this is not the case, consider the normalization of the score.


```
Example of a compatible prediction file:

Query_ID	GO_ID	Ontology	Score
A0A024RBG1	GO:0000298	M	0.5941630529283468
A0A024RBG1	GO:0003723	M	0.6652498515142261
A0A024RBG1	GO:0005515	M	0.023218466164426767
A0A024RBG1	GO:0005634	C	0.7855057534906087
A0A024RBG1	GO:0005737	C	0.7578808357708435
A0A024RBG1	GO:0005829	C	0.5129156721815427
A0A024RBG1	GO:0035556	P	0.02334179266468253
```

### Multiple Tool Prediction (on the same ground truth)

You may be interested in performing multiple benchmarks, coming from different tools on the same ground truth. Allowing you to compare directly their performance with each other.

To do so, you must set the config file to have the sections: shared, pipeline_C4 and pipeline_C5 identical. It doesn't matter if you use different config files or if you modify the same one each time.

Then, go to the preprocessing section and set the model_path to the location of your prediction file and choose a model_name, the model_name must be unique for each model (or they will overwrite each other). This will populate the same result_{run}/preprocessed_preds directory with all the preprocessed predictions and will be ready to be benchmarked. 

After the preprocessing you can the benchmark commands as usual, you just need to change the paths and model_name in the benchmark section of the config files accordingly.

**WARNING: each preprocessed predictions file is unique to each ground truth due to the different aliases!**  
Eg: if I preprocessed the prediction of a tool using the ground truth from run "test1" and then used them to benchmark against the ground truth from "test2" the results will be unreliable. To do it correctly you should repeat the preprocessing process with the new ground truth, and only then your benchmark will be reliable.


### CLI Override

You can also submit the following arguments. These will override the instruction on the config file, using the data provide with the argument itself. This will change the behaviour of the pipeline, however it will not change the config file itself.

- --cores
- --stepsize
- --cafa
- --preferred_threshold
- --stop_at_filtering
- --skip_to_filtering
- --propagate

**These arguments must be added before the main function!**

```python
python main.py --cores 16 --cafa C4 pc4 --config configs/config.template.yaml
```


### How to Make Predictions
To make the predictions themselves with your tool you should use the fasta file made by the pipeline. You can find them in results/results_{run_tag}/preprocessing_data. In this directory there will be two fasta files called  sequences_C5_nodup.fasta and sequences_C4_nodup.fasta. These are the unique fasta sequences that survived all the filtering processes in the pipeline. They contain less sequences than the original dataset (making their evaluation faster) and will allow the tool to be compared only with gene products selected for the ground truth.

These file contain: 
- the name of the sequence (a unique identifier for CAFA 4 and the UniProtKB identifier for CAFA 5)
- the taxon from which the protein was sequenced (taxon ID for cafa 5 and the standard name for CAFA 4)
- the aminoacidic sequence of the gene product itself


The prediction can be propagated by the pipeline itself setting the flag propagate: true.
You can use the fasta files to make predictions about the proteins to evaluate a tool. However, auxiliary data like protein structure is not provided and must be fetched by the tool itself, or provided in other ways.


### Typical Use Case [Python Run]

This is the typical use case for running ProBE with the Python scripts directly. 
This use case involves the creation of a diamond database with `make:db`. Then run `pc4` and/or `pc5` to create the ground truth. You can then run your own tool using the fasta files based on the ground truth. After that you need to preprocess your predictions using the command `preprocess`. After running the preprocessing you can then run the benchmarks commands themselves: `bench_general` and `bench_by_aspect` to compute the metrics.

For each run, please choose and keep constant the run_tag (in the config.yaml file) to allow each command to access the correct data.  

Sequence of commands:

make_db -> pc4/pc5 (potentially simoultaneously) -> preprocess -> bench_by_aspect/bench_general (potentially simoultaneously)

Run makedb
```python
python main.py make_db --config configs/config.template.yaml
```

Run pc4 and pc5
```python
python main.py pc4 --config configs/config.template.yaml
```
```python
python main.py pc5 --config configs/config.template.yaml
```

Make you predictions (look at [Prediction File Format](#prediction-file-format) to see the compatible formatting) and run the preprocessing
```python
python main.py preprocess --config configs/config.template.yaml
```

Run the benchmarks
```python
python main.py bench_by_aspect --config configs/config.template.yaml
```
```python
python main.py bench_general --config configs/config.template.yaml
```


### How Probe Works


Due to some quirks in the data, the CAFA 4 and CAFA 5 dataset can't share the same pipeline and must be handled differently. However the process they are subjected to is the same.

The pipelines pc4 and pc5 both start with multiple DIAMOND searches of the protein of the CAFA 4 and CAFA 5 datasets (superset_cafa4.fasta and testsuperset_cafa5.fasta). These searches are performed in multiple rounds with progressively deeper settings to find all the possible hits.

The perfects hits are selected and then an alias list is made. The alias list allows to recognize proteins with the same aminoacidic sequence but different IDs as the same gene product.

Using this alias system we then retrieve the annotations from the GOA and ascribe them to each protein. This process is repeated for the old GOA and the new GOA. These annotations are then filtered, removing duplicates and keeping only annotations from reliable experimental evidence codes (no IEA, ND or ISS).
Both these dataframe are then checked for deprecated or obsolete annotations and subsequently fixed.

The old and new dataframes are then merged keeping the evidence code of both. These evidence codes are compared to evaluate the development of the evidence of the annotation with time. Each annotation is tagged as such:
- NK: The gene product was previously unknown but now we have experimental evidence of this function
- LK1: The gene function was previously unknown for this gene product, but now it has experimental evidence
- LK2: The gene function was previously known for this gene product, but now it has experimental evidence
- KK: Previously verified knowledge

Some annotations previously regarded as experimental are lost between different releases (eg: due to retractions). So we check the ratio of missing annotations respective to the old GOA. If the number of missing annotation is too high (default: >0.2), all the annotations of that gene product are considered unreliable and is excluded from the groundtruth of the benchmark.

From the definitive benchmark the pipeline makes a fasta file containing the sequences necessary to perform the benchmark. Since not every seqeunce from the original CAFA dataset will be part of the benchmark you should use this new file to make only predictions relevant to the benchmark itself. As a positive byproduct, these sequences will be less. Speeding up the evaluation process.


#### Protein Knowledge types (NK/LK)
If a protein is annotated only by NK annotations is considered New Knowledge (NK). If a protein is annotated with LK1, LK2 (even if other KK annotation are present), it will be considered as Limited Knowledge. However, if a protein is only annotated by KK annotations it will be considered Known Knowledge and escluded from the dataset of the benchmark.

#### Preprocessing and benchmarking

The preprocess script formats the predictions as expected by the benchmarking scripts. It also automatically recognizes if the predictions regard only one CAFA or both, and then it applies the same aliasing used on the ground truth, allowing the comparison of one the other.

The benchmark produces the performance related-data based on the prediction. The output metric are precision, recall, F1-score, S measure and the ROC curve for the general measure. While the benchmark that evaluated each aspect separately computes precision, recall, FDR and F1-score. Both write a report containing the full performance information at each threshold.


### Repository Structure

Directory structure of the pipeline.

```bash
./
├── LICENSE.txt
├── README.md
├── bench_by_aspect_custom_fun.py
├── bench_general_custom_fun.py
├── configs
│   └── config.template.yaml
├── data
│   ├── dmnd_dbs
│   ├── owl_data
│   │   ├── depr.json
│   │   ├── dict_ics.json
│   │   ├── go_2025.owl
│   │   ├── graph_with_simgics_by_ontology.edgelist
│   │   └── obs.json
│   ├── raw_predictions
│   │   ├── argot_benchmark_output.txt
│   │   ├── argot_benchmark_output_filtered.txt
│   │   ├── dgp_predictions.txt
│   │   ├── output_filter_a25.py
│   │   └── output_filtered_PNNZ.txt
│   └── tmp
│       └── tmp_XXXXXXXX
│           ├── pipeline_cafa4
│           │   ├── benchmarker_cafa4
│           │   └── chunked_results_cafa4
│           ├── pipeline_cafa5
│           │   ├── benchmarker_cafa5
│           │   └── chunked_results_cafa5
│           └── preprocessing_data
├── diamond
├── diamond_chunked_cafa4.sh
├── diamond_chunked_cafa5.sh
├── diamond_chunked_fur_inv_cafa.sh
├── diamond_data
│   ├── chunked_cafa4
│   ├── chunked_cafa5
│   ├── superset_cafa4.fasta
│   └── testsuperset_cafa5.fasta
├── main.py
├── makedb.sh
├── owlLibrary3.py
├── pipeline_C4_fun.py
├── pipeline_C5_fun.py
├── preprocess_predictions_fun.py
├── requirements.txt
├── results
│   ├── results_XXXXXXX
│   │   ├── benchmark_results
│   │   ├── ground_truth
│   │   ├── knowledge_reports_cafa4
│   │   ├── knowledge_reports_cafa5
│   │   ├── preprocessed_preds
│   │   └── preprocessing_data
└── src_collection.py
```

You can choose the store the files wherever you want since the config contains the path the pipeline will be able to read it. However, it is advised to follow these instructions.
Brief description of the most notable directories:

- `data/configs`: contains the config files. You can make more copies of the template and different specs for different runs.
- `data/owl_data`: you can put here the go.owl and the GOA files. The pipeline will also make and store the files depr.json and obs.json regarding the deprecated annotations in the go and dict_ics.json, that contains the value of the information content of each annotation.
- `data/raw_predictions`:  you can put here the *unprocessed* prediction of the tool that you want to benchmark.
- `data/tmp`: this directory will be populated by different directories named tmp_{run_tag} that will contain the temporary files necessary to process the pipelines pc4 and pc5. You can set the config to delete the tmp_{run_tag} directory of each run after it has ended or to keep it. This systems allows you to have complete control over you data, proventing the pollution of the /tmp directory and the dangling intermediate problem.  
**IMPORTANT: If you're are running pc4 and pc5 simultaneously with the same run_tag you must set the flag keep_tmp: True. Or else, pc5 will finish faster and delete the directory whiLE pc4 is still relying on it!**
- `diamond_data`: contains the file necessary to perform the diamond searches on the diamond database. Don't alter it! (You can download this directory from the newest release)
- `results`: similarly to data/tmp, this directory contains a different directory for each different run, called results_{run_tag}. These directories will contain the results from any process performed by the pipeline.
    - `benchmark_results`: contains the graphs and reports regarding the computed metrics of the benchmark itself.
    - `ground_truth`: contains the dataset of the final ground truth that will be used in the benchmark.
    - `knowledge_reports_cafa4/5`: contains useful reports and graph regarding the impact of the choice of the preferred_threshold will have on the data. Consult them if you are considering on changing the threshold.
    - `preprocessed_preds`: contains the preprocessed predictions of your tool, ready to be benchmarked.
    - `preprocessing_data`: contains data computed during the processes pc4/pc5. Those are necessary to perform the preprocessing of the tool predictions.


### Components Description
There are 8 python scripts and 4 bash scripts in this repository.

There is one python script to handle each function: pipeline C4 (pc4), pipeline C5 (pc5), preprocessing the predictions (preprocess_predictions_fun), benchmarking in general and by aspect(bench_general_custom_fun, bench_by_aspect_custom_fun) and then there are three auxiliary scripts: the main which handles all the functionalities (main), a repository for most of the functions (src_collection) and a modified version of owlLibrary3 (a library for handling owl files) that can handle gzipped file.

The bash scripts interact with the tool DIAMOND to make the DIAMOND database (makedb) and perform numerous searches. Two perform the initial search in the db (one for CAFA 4 and one for CAFA 5, diamond_chunked_cafa4 and diamond_chunked_cafa5), while the other one handles the additional searches required to find all the hits for the most present gene products (diamond_chunked_fur_inv_cafa).


## Citing this Work
ProBE is developed by Gabriele Gradara at the University of Padova.

[[Email]](gabriele.gradara@studenti.unipd.it) [[Linkedin]](https://www.linkedin.com/in/gabriele-gradara/) [[Orcid]](https://orcid.org/0009-0009-0444-450X)

ProBE is research software, and we encourage you to cite it. You do not need to wait for a journal publication. You can cite the software directly using the Digital Object Identifier (DOI) provided by Zenodo.

Please cite the specific version you used for your analysis.

**To cite version v1.0.0:**

    Gabriele Gradara. (2025). ProBE: a reliable benchmarking pipeline for the evaluation of Automatic Function Prediction models (Version v1.0.0) [Computer software]. Zenodo. http://doi.org/10.5281/zenodo.PLACEHOLDER

A new DOI will be created for each new release. You can find the full list of citable versions on our Zenodo page(link).

## Acknowledgments

This work would not have been possible without the foundational datasets and tools provided by the scientific community. I gratefully acknowledge:

- Benjamin Buchfink, the developer of the DIAMOND sequence aligner for providing a high-performance tool essential to our workflow.

- Stefano Toppo and Emilio Ispano for providing me with the module owlLibrary3, for handling .owl files.